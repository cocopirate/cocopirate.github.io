{"categories":[{"title":"爬虫","uri":"https://www.antstu.com/categories/%E7%88%AC%E8%99%AB/"},{"title":"网站","uri":"https://www.antstu.com/categories/%E7%BD%91%E7%AB%99/"},{"title":"运维","uri":"https://www.antstu.com/categories/%E8%BF%90%E7%BB%B4/"}],"posts":[{"content":"关于Google Analytics Google Analytics是一个由Google所提供的网站流量统计服务，通过在网站中埋入Google Analytics追踪码，可以方便查看网站流量信息，包括来源、用户、设备、访问路径等，对于网站入门来说是非常推荐使用的工具。\n在实际使用中，会发现Google Analytics 4的Measurement ID是以G-开头的，而Hugo中的Google Analytics ID是以UA-开头的，这就导致了无法使用Google Analytics 4的问题。\nHugo使用Google Analytics 4使用方法 将Google Analyics ID（Measurement ID）放置在config.toml中的[params]中。\n# Google Analytics 4 googleAnalyticsID = \u0026#34;G-00000XXXXX\u0026#34; 创建一个名为analytics-gtag.html的文件，放置在/layouts/partials中，代码如下：\n\u0026lt;!-- Global site tag (gtag.js) - Google Analytics --\u0026gt; \u0026lt;script async src=\u0026quot;https://www.googletagmanager.com/gtag/js?id={{ .Site.Params.GoogleAnalyticsID }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', '{{ .Site.Params.GoogleAnalyticsID }}'); \u0026lt;/script\u0026gt; 在/layouts/_default/baseof.html中的\u0026lt;head\u0026gt;标签后调用这个partial，代码如下：\n\u0026lt;head\u0026gt; {{ if .Site.Params.GoogleAnalyticsID }} {{ partial \u0026quot;analytics-gtag.html\u0026quot; . }} {{ end }} ... \u0026lt;/head\u0026gt; 以上就是在Hugo中使用Google Analytics 4的方法。\n","id":0,"section":"posts","summary":"关于Google Analytics Google Analytics是一个由Google所提供的网站流量统计服务，通过在网站中埋入Google Analytics追踪码，可","tags":["建站","网站统计","Google Analytics"],"title":"Hugo中使用Google Analytics 4","uri":"https://www.antstu.com/2023/04/hugo-google-analytics/","year":"2023"},{"content":"准备条件 公网服务器：一台具有公网IP的服务器，作为服务端 官方文档：https://gofrp.org/docs/ frp下载页面：https://github.com/fatedier/frp/releases 服务端配置 下载最新版本\nwget https://github.com/fatedier/frp/releases/download/v0.44.0/frp_0.44.0_linux_386.tar.gz 解压下载文件\ntar -zxvf frp_0.44.0_linux_386.tar.gz frps和frps.ini是客户端相关文件与配置，进入解压目录配置frps.ini文件：\n[common] # 绑定端口配置 bind_port = 7000 vhost_https_port = 7022 vhost_http_port = 7080 token = 1520 # 控制面板配置 dashboard_port = 7001 dashboard_user = \u0026lt;登录账号\u0026gt; dashboard_pwd = \u0026lt;登录密码\u0026gt; enable_prometheus = true log_file = /var/log/frps.log log_level = info log_max_days = 3 在etc目录创建目录：sudo mkdir -p /etc/frp 将frps.ini复制至/etc/frp目录：sudo cp frps.ini /etc/frp 将解压文件中的frps目录复制至/usr/bin目录：sudo cp frps /usr/bin 创建并编辑frps.service文件vim /etc/systemd/system/frps.service [Unit] # 服务名称，可自定义 Description = frp server After = network.target syslog.target Wants = network.target [Service] Type = simple # 启动frps的命令，需修改为您的frps的安装路径 ExecStart = /usr/bin/frps -c /etc/frp/frps.ini [Install] WantedBy = multi-user.target 启动frp：systemctl start frps 停止frp：systemctl stop frps 重启frp：systemctl restart frps 查看frp状态：systemctl status frps 配置frps开机自启：systemctl enable frps 客户端配置 客户端指内网中需要暴露服务的设备\n从下载页面下载压缩包并解压 frpc和frpc.ini是客户端相关文件与配置 配置frpc.ini文件 [common] # 服务器的ip地址 server_addr = \u0026lt;服务器IP\u0026gt; server_port = 7000 token = 1520 # 配置http服务 [web] type = http local_port = 8000 custom_domains = \u0026lt;自定义域名\u0026gt; # 配置https服务 [test_htts2http] type = https custom_domains = \u0026lt;自定义域名\u0026gt; plugin = https2http plugin_local_addr = 127.0.0.1:8000 # HTTPS 证书相关的配置 plugin_crt_path = \u0026lt;服务器证书路径\u0026gt;.crt plugin_key_path = \u0026lt;服务器证书路径\u0026gt;.key plugin_host_header_rewrite = 127.0.0.1 plugin_header_X-From-Where = frp SSL域名证书说明\nHTTPS证书文件需放在服务器端对应的文件目录 pem转crt格式：openssl x509 -in fullchain.pem -out fullchain.crt pem转key格式：openssl rsa -in privkey.pem -out privkey.key\n客户端启动：./frpc -c frpc.ini 更改客户端host文件：\u0026lt;服务器IP\u0026gt; \u0026lt;域名\u0026gt; http访问内网8000端口服务：\u0026lt;http域名\u0026gt;:7080 https访问内网8000端口服务：\u0026lt;https域名\u0026gt;:7022 ","id":1,"section":"posts","summary":"准备条件 公网服务器：一台具有公网IP的服务器，作为服务端 官方文档：https://gofrp.org/docs/ frp下载页面：https:","tags":["frp","内网穿透"],"title":"使用frp内网穿透","uri":"https://www.antstu.com/2022/12/frp-config/","year":"2022"},{"content":"docker部署scrapyd 创建一个文件夹，并在文件夹下创建三个文件：\nDockerfile：通过docker build 制作镜像 scrapyd.conf：scrapyd的配置文件 requestments.txt：相关依赖包，项目会在scrapyd所在环境上运行 Dockerfile FROM python:3.8 WORKDIR /code RUN mkdir ./logs COPY scrapyd.conf /etc/scrapyd/ COPY requirements.txt . EXPOSE 6800 RUN pip3 install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple RUN pip3 install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple # 默认的logs位置是在 /root/logs; RUN mkdir /root/logs CMD logparser -dir ./logs \u0026amp; scrapyd requirements.txt beautifulsoup4==4.11.1 fake-useragent==0.1.11 lxml==4.8.0 pymongo==3.5.0 PyMySQL==1.0.2 pyquery==1.4.3 python-scrapyd-api==2.1.2 redis==4.3.1 requests==2.27.1 Scrapy==2.6.1 scrapy-redis==0.7.2 Scrapy-Redis-BloomFilter==0.8.1 scrapy-splash==0.8.0 scrapyd==1.3.0 logparser==0.8.2 pyopenssl==22.0.0 scrapyd.conf [scrapyd] eggs_dir = eggs logs_dir = logs items_dir = jobs_to_keep = 5 dbs_dir = dbs max_proc = 0 max_proc_per_cpu = 4 finished_to_keep = 100 poll_interval = 5.0 bind_address = 0.0.0.0 http_port = 6800 username = password = debug = off runner = scrapyd.runner jobstorage = scrapyd.jobstorage.MemoryJobStorage application = scrapyd.app.application launcher = scrapyd.launcher.Launcher webroot = scrapyd.website.Root eggstorage = scrapyd.eggstorage.FilesystemEggStorage [services] schedule.json = scrapyd.webservice.Schedule cancel.json = scrapyd.webservice.Cancel addversion.json = scrapyd.webservice.AddVersion listprojects.json = scrapyd.webservice.ListProjects listversions.json = scrapyd.webservice.ListVersions listspiders.json = scrapyd.webservice.ListSpiders delproject.json = scrapyd.webservice.DeleteProject delversion.json = scrapyd.webservice.DeleteVersion listjobs.json = scrapyd.webservice.ListJobs daemonstatus.json = scrapyd.webservice.DaemonStatus 制作镜像和启动容器 # 制作镜像 docker build -t scrapyd . # 启动容器 docker run -d -p 6800:6800 --name scrapyd scrapyd docker部署scrapyweb 创建一个文件夹，并在文件夹下创建三个文件：\nDockerfile：通过docker build 制作镜像 scrapydweb_settings_v10.py：scrapydweb的配置文件，为方便修改使用挂载方式 Dockerfile FROM python:3.10 WORKDIR /code EXPOSE 5000 RUN pip3 install Werkzeug==2.0.3 -i https://pypi.tuna.tsinghua.edu.cn/simple RUN pip3 install scrapydweb -i https://pypi.tuna.tsinghua.edu.cn/simple CMD scrapydweb scrapydweb_settings_v10.py 在宿主机先通过pip install scrapydweb安装scrapydweb，安装后执行scrapydweb生成scrapydweb_settings_v10.py文件，修改文件中的SCRAPYD_SERVERS配置\n# 其他配置... SCRAPYD_SERVERS = [ 'my_scrapyd:6800' # 使用容器的链接配置SCRAPYD_SERVERS地址 ] # 其他配置... 制作镜像和启动容器 将已创建的scrapyd容器与scrapydweb容器链接并命名为my_scrapyd； 将宿主机的/root/scrapydweb目录挂载至容器的code目录，方便配置scrapydweb_settings_v10.py文件。 docker build -t scrapydweb . docker run -d -p 5000:5000 --link scrapyd:my_scrapyd -v /root/scrapydweb:/code --name scrapydweb scrapydweb 其他事项 访问scrapyweb如果出现Http400处理方式：\npip install flask==2.0.2 pip install flask-compress==1.12 在容器中通过scrapydweb命令重启即可。\n","id":2,"section":"posts","summary":"docker部署scrapyd 创建一个文件夹，并在文件夹下创建三个文件： Dockerfile：通过docker build 制作镜像 scrapyd.co","tags":["scrapy","爬虫","docker","scrapyd","scrapyweb"],"title":"Docker部署Scrapyd+Scrapyweb","uri":"https://www.antstu.com/2022/11/docker-scrapyd-scrapyweb/","year":"2022"}],"tags":[{"title":"docker","uri":"https://www.antstu.com/tags/docker/"},{"title":"frp","uri":"https://www.antstu.com/tags/frp/"},{"title":"Google Analytics","uri":"https://www.antstu.com/tags/google-analytics/"},{"title":"scrapy","uri":"https://www.antstu.com/tags/scrapy/"},{"title":"scrapyd","uri":"https://www.antstu.com/tags/scrapyd/"},{"title":"scrapyweb","uri":"https://www.antstu.com/tags/scrapyweb/"},{"title":"内网穿透","uri":"https://www.antstu.com/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"title":"建站","uri":"https://www.antstu.com/tags/%E5%BB%BA%E7%AB%99/"},{"title":"爬虫","uri":"https://www.antstu.com/tags/%E7%88%AC%E8%99%AB/"},{"title":"网站统计","uri":"https://www.antstu.com/tags/%E7%BD%91%E7%AB%99%E7%BB%9F%E8%AE%A1/"}]}